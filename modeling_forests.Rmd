---
title: "Modeling Exploration Practical 1"
author: "Elling Payne"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(fig.align = "center");
```

```{r}
library(tidyverse);
#library(caret);
set.seed(112358);
load("out/preprocessing/data/youth_nsduh_2023_transformed.Rdata");

test.indices <- sample(1:nrow(youth_nsduh_2023_transformed),
                       size=0.2*nrow(youth_nsduh_2023_transformed));
test.data <- youth_nsduh_2023_transformed[test.indices,];
train.data <- youth_nsduh_2023_transformed[-test.indices,];

test_errs <- list();
```

### Problem 1: Predicting whether a youth has used either cigarettes or smokeless nicotine products

An initial exploration seems to reveal that the models are not very interesting when other substance use variables are included, since these tend to over shadow others. A random forest model may be able to handle this better than a simple tree or bagged model.

```{r}
# remove cig/smkls and irrelevant columns, pick all other substance columns for now,
# will filter later and compare
other_substance_cols <- c("IRALCFY", "IRMJFY", "IRALCFM", "IRMJFM", "IRALCAGE",
                          "IRMJAGE", "MRJFLAG", "ALCFLAG", "ALCYDAYS", "MRJYDAYS",
                          "ALCMDAYS", "MRJMDAYS"); 
prob1_data.train <- train.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "TOBFLAG")));
prob1_data.test <- test.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "TOBFLAG")));
write.csv(prob1_data.train, "out/analysis/problem1/data/train_data_problem1.csv",
          row.names=FALSE);
write.csv(prob1_data.test, "out/analysis/problem1/data/test_data_problem1.csv",
          row.names=FALSE);
```

```{r}
# simple trees
library(tree);


fit.tree <- tree(TOBFLAG ~ ., data = prob1_data.train);
cv.result <- cv.tree(fit.tree, FUN=prune.misclass);
print(which.min(cv.result$dev));
pruned.tree <- prune.tree(fit.tree, best=3);
print(summary(pruned.tree));
png("out/analysis/problem1/plots/cverror_vs_pruning_prob1.png");
plot(cv.result$size, cv.result$dev, main="cv error vs tree size");
dev.off();
png("out/analysis/problem1/plots/tree_pruned_prob1.png");
plot(pruned.tree);
text(pruned.tree, pretty=0);
dev.off();
png("out/analysis/problem1/plots/tree_full_prob1.png");
plot(fit.tree);
text(fit.tree, pretty=0);
dev.off();
print(cv.result);

pred <- predict(fit.tree, newdata=prob1_data.test, type="class");
conf.mat <- table(pred, prob1_data.test$TOBFLAG);
testerr <- (conf.mat[1,2] + conf.mat[2,1]) / sum(conf.mat);
test_errs <- rbind(test_errs, data.frame(problem=1, model_type="tree",
                                         test_error=testerr, error_type="error_rate"));
```
```{r}
# bagged model
library(randomForest);
fit.bag <- randomForest(TOBFLAG ~ ., data=prob1_data.train, mtry=ncol(prob1_data.train)-1,
                       importance=TRUE);
png("out/analysis/problem1/plots/error_vs_size_bagging_problem1.png");
plot(fit.bag, main="OOB Error (Total and by Group)");

legend("topright", legend = c("Total", "No Tobacco", "Tobacco"), col = 1:ncol(fit.bag$err.rate),
       lty = 1:ncol(fit.bag$err.rate), cex = 0.8);
dev.off();

pred <- predict(fit.bag, newdata=prob1_data.test, type="class");
conf.mat <- table(pred, prob1_data.test$TOBFLAG);
testerr <- (conf.mat[1,2] + conf.mat[2,1]) / sum(conf.mat);
test_errs <- rbind(test_errs, data.frame(problem=1, model_type="bagging",
                                         test_error=testerr, error_type="error_rate"));
```
```{r}
varImpPlot(fit.bag);
imp_bag <- importance(fit.bag)[order(importance(fit.bag)[, 4], decreasing=TRUE),];
write.csv(imp_bag, "out/analysis/problem1/data/rel_import_bagging_problem1.csv",
          row.names=TRUE);

```

```{r}
tune.result <- tuneRF(x=prob1_data.train %>% select(-TOBFLAG),
                      y=prob1_data.train$TOBFLAG,
                      mtryStart=floor(sqrt(ncol(prob1_data.train)-1)),
                      stepFactor=1.2,
                      improve=0.001,
                      ntreeTry=100,);
best.mtry <- tune.result[which.min(tune.result[,2]), 1];
print(paste0("Best choice of mtry based on OOB error is ", best.mtry));
best.rf <- randomForest(TOBFLAG ~ ., data=prob1_data.train, mtry=best.mtry,
                           importance=TRUE);
png("out/analysis/problem1/plots/error_vs_size_randforest_problem1.png");
plot(best.rf, main="OOB Error (Total and by Group)");

legend("topright", legend = c("Total", "No Tobacco", "Tobacco"), col = 1:ncol(best.rf$err.rate),
       lty = 1:ncol(best.rf$err.rate), cex = 0.8);
dev.off();

pred <- predict(best.rf, newdata=prob1_data.test, type="class");
conf.mat <- table(pred, prob1_data.test$TOBFLAG);
testerr <- (conf.mat[1,2] + conf.mat[2,1]) / sum(conf.mat);
test_errs <- rbind(test_errs, data.frame(problem=1, model_type="random_forest",
                                         test_error=testerr, error_type="error_rate"));
```
```{r}
varImpPlot(best.rf);
relimp <- importance(best.rf)[order(importance(fit.bag)[, 4], decreasing=TRUE),];
write.csv(relimp, "out/analysis/problem1/data/rel_import_randforest_problem1.csv",
          row.names=TRUE);
```


### Problem 2: Predicting whether a youth drinks alcohol never, seldom, or often

```{r}
# remove alc and irrelevant columns, pick all other substance columns for now,
# will filter later and compare
other_substance_cols <- c("IRMJFY", "IRCIGFM", "IRSMKLSS30N", "IRMJFM", "IRCIGAGE",
                          "IRSMKLSSTRY", "IRMJAGE", "MRJFLAG", "TOBFLAG", "MRJYDAYS",
                          "MRJMDAYS", "CIGMDAYS", "SMKLSMDAYS"); 
prob2_data.train <- train.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "ALCYDAYS"))) %>%
  mutate(ALCYFREQ = as.factor(ifelse(ALCYDAYS == 0, "Never",
                              ifelse(ALCYDAYS == 1, "Seldom", "Moderate/Often")))) %>%
                    select(-ALCYDAYS);
prob2_data.test <- test.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "ALCYDAYS"))) %>%
  mutate(ALCYFREQ = as.factor(ifelse(ALCYDAYS == 0, "Never",
                              ifelse(ALCYDAYS == 1, "Seldom", "Moderate/Often")))) %>%
                    select(-ALCYDAYS);
write.csv(prob2_data.train, "out/analysis/problem2/data/train_data_problem2.csv",
          row.names=FALSE);
write.csv(prob2_data.test, "out/analysis/problem2/data/test_data_problem2.csv",
          row.names=FALSE);
```

```{r}
# simple trees
library(tree);


fit.tree <- tree(ALCYFREQ ~ ., data = prob2_data.train);
cv.result <- cv.tree(fit.tree, FUN=prune.misclass);
print(which.min(cv.result$dev));
pruned.tree <- prune.tree(fit.tree, best=3);
print(summary(pruned.tree));
png("out/analysis/problem2/plots/cverror_vs_pruning_prob2.png");
plot(cv.result$size, cv.result$dev, main="cv error vs tree size");
dev.off();
png("out/analysis/problem2/plots/tree_pruned_prob2.png");
plot(pruned.tree);
text(pruned.tree, pretty=0);
dev.off();
png("out/analysis/problem2/plots/tree_full_prob2.png");
plot(fit.tree);
text(fit.tree, pretty=0);
dev.off();
print(cv.result);

pred <- predict(fit.tree, newdata=prob2_data.test, type="class");
conf.mat <- table(pred, prob2_data.test$ALCYFREQ);
testerr <- (conf.mat[1,2] + conf.mat[2,1]) / sum(conf.mat);
test_errs <- rbind(test_errs, data.frame(problem=2, model_type="tree",
                                         test_error=testerr, error_type="error_rate"));
```


```{r}
# random forests
tune.result <- tuneRF(x=prob2_data.train %>% select(-ALCYFREQ),
                      y=prob2_data.train$ALCYFREQ,
                      mtryStart=floor(sqrt(ncol(prob2_data.train)-1)),
                      stepFactor=1.2,
                      improve=0.001,
                      ntreeTry=100,);
best.mtry <- tune.result[which.min(tune.result[,2]), 1];
print(paste0("Best choice of mtry based on OOB error is ", best.mtry));
best.rf <- randomForest(ALCYFREQ ~ ., data=prob2_data.train, mtry=best.mtry,
                           importance=TRUE);
png("out/analysis/problem2/plots/error_vs_size_randforest_problem2.png");
plot(best.rf, main="OOB Error (Total and by Group)");

legend("topright", legend = colnames(best.rf$err.rate), col = 1:ncol(best.rf$err.rate),
       lty = 1:ncol(best.rf$err.rate), cex = 0.8);
dev.off();

pred <- predict(best.rf, newdata=prob2_data.test, type="class");
conf.mat <- table(pred, prob2_data.test$ALCYFREQ);
testerr <- (conf.mat[1,2] + conf.mat[2,1]) / sum(conf.mat);
test_errs <- rbind(test_errs, data.frame(problem=2, model_type="random_forest",
                                         test_error=testerr, error_type="error_rate"));
```

```{r}
varImpPlot(best.rf);
rel_imp <- importance(best.rf)[order(importance(fit.rf)[, 4], decreasing=TRUE),];
print(rel_imp);
write.csv(rel_imp, "out/analysis/problem2/data/rel_imp_randforest_problem2.csv",
          row.names = TRUE);
```


### Problem 3: Predicting the number of days a youth used marijuana in the past year

```{r}
# remove mj and irrelevant columns, pick all other substance columns for now,
# will filter later and compare
other_substance_cols <- c("IRALCFY", "IRCIGFM", "IRSMKLSS30N", "IRALCFM", "IRMJFM",
                          "IRCIGAGE", "IRSMKLSSTRY", "IRALCAGE", "ALCFLAG", "TOBFLAG",
                          "ALCYDAYS", "ALCMDAYS", "CIGMDAYS", "SMKLSMDAYS"); 
prob3_data.train <- train.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "IRMJFY")));
prob3_data.test <- test.data %>%
  select(all_of(c(demographic_cols, youth_experience_cols, "IRMJFY")));
write.csv(prob3_data.train, "out/analysis/problem3/data/train_data_problem3.csv",
          row.names=FALSE);
write.csv(prob3_data.test, "out/analysis/problem3/data/test_data_problem3.csv",
          row.names=FALSE);
```

```{r}
# simple tree
library(tree);


fit.tree <- tree(IRMJFY ~ ., data = prob3_data.train);
cv.result <- cv.tree(fit.tree, FUN=prune.tree);
print(which.min(cv.result$dev));
pruned.tree <- prune.tree(fit.tree, best=6);
print(summary(pruned.tree));
png("out/analysis/problem3/plots/cverror_vs_pruning_prob3.png");
plot(cv.result$size, cv.result$dev, main="cv error vs tree size");
dev.off();
png("out/analysis/problem3/plots/tree_pruned_prob3.png");
plot(pruned.tree);
text(pruned.tree, pretty=0);
dev.off();
png("out/analysis/problem3/plots/tree_full_prob3.png");
plot(fit.tree);
text(fit.tree, pretty=0);
dev.off();
print(cv.result);

pred <- predict(fit.tree, newdata=prob3_data.test);
testerr <- mean((pred - prob3_data.test$IRMJFY)^2);
test_errs <- rbind(test_errs, data.frame(problem=3, model_type="tree",
                                         test_error=testerr, error_type="mse"));
```


```{r}
# boosted models
tune.grid <- expand.grid( n.trees = c(25, 50, 100, 150),
                          interaction.depth = 1:3,
                          shrinkage =c(0.01, 0.05, 0.1, 0.25));
```
```{r}
library(gbm);
param_tune.results <- data.frame(tune.grid);
cv.scores <- numeric(nrow(tune.grid));
train.errs <- numeric(nrow(tune.grid));
gbmtest.errs <- numeric(nrow(tune.grid));
for (r in 1:nrow(tune.grid)) {
  fit.gbm <- gbm(IRMJFY ~ ., data=prob3_data.train, distribution="gaussian",
                 cv.folds=5, verbose=FALSE,
                 n.trees = tune.grid[r, "n.trees"],
                 interaction.depth = tune.grid[r, "interaction.depth"],
                 shrinkage = tune.grid[r, "shrinkage"]);
  cv.scores[r] <- fit.gbm$cv.error[length(fit.gbm$cv.error)];
  train.errs[r] <- fit.gbm$train.error[length(fit.gbm$train.error)];
  pred <- predict(fit.gbm, newdata=prob3_data.test, n.trees=tune.grid[r, "n.trees"]);
  gbmtest.errs[r] <- mean((pred - prob3_data.test$IRMJFY)^2);
}
param_tune.results$cv.error <- cv.scores; # for choosing
param_tune.results$train.error <- train.errs; # for plotting
param_tune.results$test.error <- gbmtest.errs; # for plotting
```
```{r}
best.params <- param_tune.results[which.min(cv.scores),];
best.gbm <- gbm(IRMJFY ~ ., data=prob3_data.train, distribution="gaussian",
                verbose=FALSE,
                n.trees = best.params[["n.trees"]],
                interaction.depth = best.params[["interaction.depth"]],
                shrinkage = best.params[["shrinkage"]]);

pred <- predict(best.gbm, newdata=prob3_data.test, n.trees=best.params[["n.trees"]]);
testerr <- mean((pred - prob3_data.test$IRMJFY)^2);
test_errs <- rbind(test_errs, data.frame(problem=3, model_type="boosting",
                                         test_error=testerr, error_type="mse"));

print(best.params);
write.csv(summary(best.gbm), "out/analysis/problem3/data/rel_import_boosting_problem3.csv",
          row.names = TRUE);

write.csv(test_errs, "out/analysis/Validation_set_error_all_problems_models.csv",
          row.names = FALSE);
```

```{r}
write.csv(param_tune.results, "out/analysis/problem3/params_and_errs_boosting_problem3.csv",
          row.names = FALSE);
tuners <- c("n.trees", "interaction.depth", "shrinkage");
for (param in tuners) {
  plot_data <- param_tune.results %>%
    mutate(thisparam=param_tune.results[[param]]) %>% select(-all_of(tuners)) %>%
    pivot_longer(cols=c("cv.error", "train.error", "test.error"),
                 names_to = "Error_Type",
                 values_to = "Error") %>%
    mutate(Error_Type = as.factor(Error_Type));
  print(sum(plot_data$Error));
  plt <- ggplot(data=plot_data, mapping=aes(x=thisparam, y=Error,
                                            color=Error_Type,
                                            shape=Error_Type)) +
    geom_point() +
    labs(title = paste0("Error vs ", param),
         x=param, y="Error",
         color="Error Type",
         shape="Error Type") +
    scale_color_manual(values = c("darkgrey", "skyblue", "coral"));
  print(plt);
  ggsave(filename=paste0("out/analysis/problem3/plots/", param,
                         "_vs_error_boosting_problem3.png"), plot=plt);
}
```



